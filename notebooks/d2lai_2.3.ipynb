{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tensor can be defined by three attributes - dimensions, order, dtype\n",
    "\n",
    "ex = torch.randn(3, 2)\n",
    "ex.shape, ex.ndim, ex.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### order is number of axes\n",
    "\n",
    "# scalar is 0\n",
    "x_scalar = torch.tensor(3.0)\n",
    "\n",
    "# vector is 1\n",
    "x_vector = torch.tensor([3.0, 4.0])\n",
    "\n",
    "# matrix is 2\n",
    "x_matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# tensor is n\n",
    "x_tensor = torch.rand(2, 3, 1, 2, 3)\n",
    "\n",
    "print(x_scalar.ndim, x_vector.ndim, x_matrix.ndim, x_tensor.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### properties - element wise operations keep same shape\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "y = torch.randn(3, 4)\n",
    "\n",
    "(x+y).shape, (x/y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor * scalar is same shape as tensor\n",
    "\n",
    "(x * 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reductions vs non-reductions\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "print(x)\n",
    "\n",
    "# when you define axis, you drop that axis\n",
    "x.sum(), x.mean(), x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keepdim for broadcasting; now colums sum up to 1\n",
    "\n",
    "x / x.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element wise product (hadamard) vs. dot product vs mat mul\n",
    "\n",
    "\n",
    "# element wise product\n",
    "print('element wise product')\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "\n",
    "print(a * b)\n",
    "print('\\n----------------')\n",
    "\n",
    "print('dot product\\n')\n",
    "\n",
    "# dot product\n",
    "print(torch.dot(a, b) == (a*b).sum())\n",
    "\n",
    "print('\\n----------------')\n",
    "print('mat mul')\n",
    "\n",
    "# matmul is dot product between rows of X and columns of Y\n",
    "X = torch.randn((3, 4))\n",
    "Y = torch.randn((4, 5))\n",
    "\n",
    "x_rows, x_cols = X.shape\n",
    "y_rows, y_cols = Y.shape\n",
    "\n",
    "Z = torch.zeros((x_rows, y_cols))\n",
    "for i in range(x_rows):\n",
    "    for j in range(y_cols):\n",
    "            Z[i, j] = torch.dot(X[i, :], Y[:, j])\n",
    "\n",
    "Z2 = X @ Y\n",
    "\n",
    "print(torch.allclose(Z, Z2, atol=1e-6))\n",
    "\n",
    "Z, Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element wise product\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product\n",
    "torch.dot(a, b) == (a*b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matmul is dot product between rows of X and columns of Y\n",
    "X = torch.randn((3, 4))\n",
    "Y = torch.randn((4, 5))\n",
    "\n",
    "x_rows, x_cols = X.shape\n",
    "y_rows, y_cols = Y.shape\n",
    "\n",
    "Z = torch.zeros((x_rows, y_cols))\n",
    "for i in range(x_rows):\n",
    "    for j in range(y_cols):\n",
    "            Z[i, j] = torch.dot(X[i, :], Y[:, j])\n",
    "\n",
    "Z2 = X @ Y\n",
    "\n",
    "print(torch.allclose(Z, Z2, atol=1e-6))\n",
    "\n",
    "Z, Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product as linear transformation\n",
    "\n",
    "A = torch.ones(2, 3) # (2,3)\n",
    "v = torch.tensor([1, 1, 1], dtype=torch.float32) # (3,)\n",
    "\n",
    "print(A.shape, v.shape)\n",
    "\n",
    "A @ v # (2,3) @ (3,) -> (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norms\n",
    "dummy = torch.tensor([3, 4], dtype=torch.float32)\n",
    "\n",
    "# p = 1 is manhattan, p = 2 is euclidean\n",
    "torch.norm(dummy, p=1), torch.norm(dummy, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### l1 and l2\n",
    "torch.abs(dummy).sum(), (dummy ** 2).sum() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frob norm\n",
    "dummy_matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "print(dummy_matrix.shape)\n",
    "\n",
    "torch.norm(dummy_matrix, p=2), (dummy_matrix ** 2).sum() ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.13 exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose of transpose\n",
    "a = torch.randn((4, 5))\n",
    "\n",
    "# transpose of transpose is the original matrix\n",
    "a.T.T == a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose properties\n",
    "a = torch.randn((4, 5))\n",
    "b = torch.randn((4, 5))\n",
    "\n",
    "(a.T + b.T) == (a + b).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmmetric\n",
    "A = torch.randn((2, 2))\n",
    "\n",
    "(A + A.T) == (A + A.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A + A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T\n",
    "\n",
    "A, A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(2, 3, 4)\n",
    "Y = torch.randn(3, 9)\n",
    "\n",
    "# len is dimensions of leading dim\n",
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction division - fails because broadcasting is wrong\n",
    "\n",
    "A = torch.randn(2, 3)\n",
    "\n",
    "A / A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction division\n",
    "\n",
    "A = torch.randn(2, 3)\n",
    "\n",
    "A / A.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (torch.arange(24, dtype=torch.float32) + 1 ) * 2 - 5\n",
    "X = X.reshape((2, 3, 4))\n",
    "X\n",
    "\n",
    "# 3x4, 2x4, 2x3\n",
    "X.sum(axis=0).shape, X.sum(axis=1).shape, X.sum(axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frobenius norm\n",
    "torch.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2**10, 2**16)\n",
    "B = torch.randn(2**16, 2**5)\n",
    "C = torch.randn(2**5, 2**14)\n",
    "\n",
    "# is there a difference btween AB * C or A * BC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit 100\n",
    "\n",
    "res = (A@B) @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit 100\n",
    "\n",
    "res = A @ (B@C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.mps\n",
    "import time\n",
    "\n",
    "# Assuming 'mps_device' is defined as your MPS device\n",
    "mps_device = torch.device('mps')\n",
    "\n",
    "# Initialize large matrices with Gaussian random variables\n",
    "A = torch.randn(2**10, 2**16, device=mps_device)\n",
    "B = torch.randn(2**16, 2**5, device=mps_device)\n",
    "C = torch.randn(2**5, 2**14, device=mps_device)\n",
    "\n",
    "# Function to measure the current allocated memory\n",
    "def current_memory():\n",
    "    return torch.mps.current_allocated_memory()\n",
    "\n",
    "# Function to perform matrix multiplication and measure time and memory\n",
    "def multiply_and_measure(A, B, C):\n",
    "    start_time = time.time()\n",
    "    memory_before = current_memory()\n",
    "    AB = torch.matmul(A, B)\n",
    "    ABC = torch.matmul(AB, C)\n",
    "    memory_after = current_memory()\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    memory_used = memory_after - memory_before\n",
    "    return time_taken, memory_used\n",
    "\n",
    "# Number of iterations for averaging\n",
    "num_iterations = 100\n",
    "\n",
    "# Start MPS profiler for performance tracing\n",
    "torch.mps.profiler.start()\n",
    "\n",
    "# Compute (A B) C multiple times\n",
    "total_time_AB_C = 0\n",
    "total_memory_AB_C = 0\n",
    "for _ in range(num_iterations):\n",
    "    time_taken, memory_used = multiply_and_measure(A, B, C)\n",
    "    total_time_AB_C += time_taken\n",
    "    total_memory_AB_C += memory_used\n",
    "\n",
    "# Compute A (B C) multiple times\n",
    "total_time_A_BC = 0\n",
    "total_memory_A_BC = 0\n",
    "for _ in range(num_iterations):\n",
    "    time_taken, memory_used = multiply_and_measure(A, B, C)\n",
    "    total_time_A_BC += time_taken\n",
    "    total_memory_A_BC += memory_used\n",
    "\n",
    "# Stop MPS profiler after performance tracing\n",
    "torch.mps.profiler.stop()\n",
    "\n",
    "# Calculate and print the average timing and memory results\n",
    "average_time_AB_C = total_time_AB_C / num_iterations\n",
    "average_memory_AB_C = total_memory_AB_C / num_iterations\n",
    "average_time_A_BC = total_time_A_BC / num_iterations\n",
    "average_memory_A_BC = total_memory_A_BC / num_iterations\n",
    "\n",
    "print(f\"Average time for (AB)C: {average_time_AB_C} seconds\")\n",
    "print(f\"Average memory for (AB)C: {average_memory_AB_C} bytes\")\n",
    "print(f\"Average time for A(BC): {average_time_A_BC} seconds\")\n",
    "print(f\"Average memory for A(BC): {average_memory_A_BC} bytes\")\n",
    "\n",
    "# # Empty cache if needed\n",
    "# torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of scalar multiplications for each matrix multiplication\n",
    "# Calculations for BC\n",
    "calculations_BC = 2**16 * 2**5 * 2**14\n",
    "\n",
    "# Calculations for A(BC)\n",
    "calculations_A_BC = 2**10 * 2**5 * 2**14\n",
    "\n",
    "# Total calculations for A(BC)\n",
    "total_calculations_A_BC = calculations_BC + calculations_A_BC\n",
    "total_calculations_A_BC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.mps\n",
    "import time\n",
    "\n",
    "# Assuming 'mps_device' is defined as your MPS device\n",
    "mps_device = torch.device('mps')\n",
    "\n",
    "# Initialize large matrices with Gaussian random variables\n",
    "A = torch.randn(2**10, 2**16, device=mps_device)\n",
    "B = torch.randn(2**16, 2**5, device=mps_device)\n",
    "C = torch.randn(2**5, 2**14, device=mps_device)\n",
    "\n",
    "# Function to measure the current allocated memory\n",
    "def current_memory():\n",
    "    return torch.mps.current_allocated_memory()\n",
    "\n",
    "# Function to perform matrix multiplication and measure time and memory\n",
    "def multiply_and_measure(A, B, C, operation_order):\n",
    "    start_time = time.time()\n",
    "    memory_before = current_memory()\n",
    "    \n",
    "    if operation_order == \"A_BC\":\n",
    "        BC = torch.matmul(B, C)\n",
    "        result = torch.matmul(A, BC)\n",
    "    elif operation_order == \"AB_C\":\n",
    "        AB = torch.matmul(A, B)\n",
    "        result = torch.matmul(AB, C)\n",
    "        \n",
    "    memory_after = current_memory()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    time_taken = end_time - start_time\n",
    "    memory_used = memory_after - memory_before\n",
    "    \n",
    "    return time_taken, memory_used\n",
    "\n",
    "# Number of iterations for averaging\n",
    "num_iterations = 100\n",
    "\n",
    "# Start MPS profiler for performance tracing\n",
    "torch.mps.profiler.start()\n",
    "\n",
    "# Compute A (B C) multiple times\n",
    "total_time_A_BC = 0\n",
    "total_memory_A_BC = 0\n",
    "for _ in range(num_iterations):\n",
    "    time_taken, memory_used = multiply_and_measure(A, B, C, \"A_BC\")\n",
    "    total_time_A_BC += time_taken\n",
    "    total_memory_A_BC += memory_used\n",
    "\n",
    "# Compute (A B) C multiple times\n",
    "total_time_AB_C = 0\n",
    "total_memory_AB_C = 0\n",
    "for _ in range(num_iterations):\n",
    "    time_taken, memory_used = multiply_and_measure(A, B, C, \"AB_C\")\n",
    "    total_time_AB_C += time_taken\n",
    "    total_memory_AB_C += memory_used\n",
    "\n",
    "# Stop MPS profiler after performance tracing\n",
    "torch.mps.profiler.stop()\n",
    "\n",
    "# Calculate and print the average timing and memory results\n",
    "average_time_A_BC = total_time_A_BC / num_iterations\n",
    "average_memory_A_BC = total_memory_A_BC / num_iterations\n",
    "average_time_AB_C = total_time_AB_C / num_iterations\n",
    "average_memory_AB_C = total_memory_AB_C / num_iterations\n",
    "\n",
    "print(f\"Average time for A(BC): {average_time_A_BC} seconds\")\n",
    "print(f\"Average memory for A(BC): {average_memory_A_BC} bytes\")\n",
    "print(f\"Average time for (AB)C: {average_time_AB_C} seconds\")\n",
    "print(f\"Average memory for (AB)C: {average_memory_AB_C} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize large matrices with Gaussian random variables\n",
    "A = torch.randn(2**10, 2**16, device=mps_device)\n",
    "B = torch.randn(2**16, 2**5, device=mps_device)\n",
    "C = torch.randn(2**5, 2**14, device=mps_device)\n",
    "\n",
    "### (AB)C\n",
    "# Calculations for AB\n",
    "calculations_AB = 2**10 * 2**16 * 2**5\n",
    "\n",
    "# Calculations for (AB)C\n",
    "calculations_AB_C = 2**10 * 2**5 * 2**14\n",
    "\n",
    "# Total calculations for A(BC)\n",
    "total_calculations_AB_C = calculations_AB + calculations_AB_C\n",
    "total_calculations_AB_C\n",
    "\n",
    "### A(BC)\n",
    "# Number of scalar multiplications for each matrix multiplication\n",
    "# Calculations for BC\n",
    "calculations_BC = 2**16 * 2**5 * 2**14\n",
    "\n",
    "# Calculations for A(BC)\n",
    "calculations_A_BC = 2**10 * 2**5 * 2**14\n",
    "\n",
    "# Total calculations for A(BC)\n",
    "total_calculations_A_BC = calculations_BC + calculations_A_BC\n",
    "total_calculations_A_BC\n",
    "\n",
    "print(f'AB_C vs. A_BC')\n",
    "total_calculations_AB_C / 1000000000, total_calculations_A_BC / 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C = torch.randn(100, 200), torch.randn(100, 200), torch.randn(100, 200)\n",
    "\n",
    "A.shape, B.shape, C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked\n",
    "stacked_abc = torch.stack([A, B, C])\n",
    "print(stacked_abc.shape)\n",
    "\n",
    "# or add as three dim\n",
    "torch.stack([A, B, C], dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out B\n",
    "torch.stack([A, B, C], dim=2)[:, :, 1] == B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
